{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning by applying code\n",
    "  \n",
    "From [\"Finding similar images using Deep learning and Locality Sensitive Hashing\"](https://towardsdatascience.com/finding-similar-images-using-deep-learning-and-locality-sensitive-hashing-9528afee02f5) on towardsdatascience.com.\n",
    "  \n",
    ">\"A simple walkthrough on finding similar images through image embedding by a ResNet 34 using FastAI & Pytorch. Also doing fast semantic similarity search in huge image embeddings collections.\"\n",
    "  \n",
    "  \n",
    "> The process to achieve the above result can be broken down in these few steps -\n",
    "1. Transfer learning from a ResNet-34 model(trained on ImageNet) to detect 101 classes in Caltech-101 dataset using FastAI and Pytorch.\n",
    "2. Take the output of second last fully connected layer from trained ResNet 34 model to get embedding for all 9,144 Caltech-101 images.\n",
    "3. Use Locality Sensitive hashing to create LSH hashing for our image embedding which enables fast approximate nearest neighbor search\n",
    "4. Then given an image, we can convert it into image embedding using our trained model and then search similar images using Approximate nearest neighbor on Caltech-101 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data understanding and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The first exercise in our project is to obtain a deep learning network which can classify these categories accurately. For this task, we will use a pre-trained ResNet 34 network which is trained on the ImageNet database and transfer learn it to classify 101 categories of Caltech-101 database using Pytorch 1.0 and FastAI library.\"\n",
    "\n",
    "Testing my own dataset - ~120 images, 8 categories. This could be terrible lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:44.138067Z",
     "start_time": "2019-01-31T21:12:41.459607Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks.hooks import *\n",
    "from lshash_2 import LSHash\n",
    "#from lshash import LSHash\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../repo/data/model_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:21:24.063782Z",
     "start_time": "2019-01-31T20:21:23.345797Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get_Transforms is a fastai function\n",
    "tfms = get_transforms(\n",
    "    do_flip=False, \n",
    "    flip_vert=False, \n",
    "    max_rotate=0, \n",
    "    max_lighting=0, \n",
    "    max_zoom=1, \n",
    "    max_warp=0\n",
    ")\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_rand_pct(0.2)\n",
    "        .label_from_folder()\n",
    "        .transform(tfms=tfms, size=224)\n",
    "        .databunch(bs=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of classes {0}'.format(data.c))\n",
    "# print(data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Train dataset size: {0}'.format(len(data.train_ds.x)))\n",
    "# print('Test dataset size: {0}'.format(len(data.valid_ds.x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show sample data\n",
    "data.show_batch(rows=3, figsize=(10,6), hide_axis=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:21:26.903569Z",
     "start_time": "2019-01-31T20:21:26.058716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Creating the model\n",
    "learn = cnn_learner(data, models.resnet34, pretrained=True, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:22:07.710266Z",
     "start_time": "2019-01-31T20:21:27.295136Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### This took about 23 mins\n",
    "## Finding Ideal learning late\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:26:59.789144Z",
     "start_time": "2019-01-31T20:22:10.638669Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fitting 5 epochs <--- what does this mean??\n",
    "learning_rate = 1e-2 # choose based on a loss ~0.25\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:26:59.927080Z",
     "start_time": "2019-01-31T20:26:59.791623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Saving stage 1 model weights\n",
    "learn.save('stg1-rn34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:27:40.233258Z",
     "start_time": "2019-01-31T20:27:03.973846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Unfreeing layer and finding ideal learning rate\n",
    "learn.unfreeze()\n",
    "learn.lr_find() # this took 30 mins for 100 images\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:34:52.779416Z",
     "start_time": "2019-01-31T20:28:08.975096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Fitting 5 epochs\n",
    "learn.fit_one_cycle(5, slice(1e-5, 1e-2/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:56:19.520966Z",
     "start_time": "2019-01-31T20:56:19.233898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Saving model weights\n",
    "learn.save('stg2-rn34')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extracting Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a hook right after convolutional part of resnet 50 and max pooling layer which generates a 4096 length vector for a particular image of 256*256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:37:30.953311Z",
     "start_time": "2019-01-31T20:37:30.931046Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this is a hook (learned about it here: \n",
    "# https://forums.fast.ai/t/how-to-find-similar-images-based-on-final-embedding-layer/16903/13)\n",
    "# hooks are used for saving intermediate computations\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): \n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "        self.features = None\n",
    "    def hook_fn(self, module, input, output): \n",
    "        out = output.detach().cpu().numpy()\n",
    "        if isinstance(self.features, type(None)):\n",
    "            self.features = out\n",
    "        else:\n",
    "            self.features = np.row_stack((self.features, out))\n",
    "    def remove(self): \n",
    "        self.hook.remove()\n",
    "        \n",
    "sf = SaveFeatures(learn.model[1][5]) ## Output before the last FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:42:10.437130Z",
     "start_time": "2019-01-31T20:42:02.972854Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## By running this, feature vectors would be saved in sf variable initated above\n",
    "_= learn.get_preds(data.train_ds)\n",
    "_= learn.get_preds(DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Converting in a dictionary of {img_path:featurevector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:42:26.967051Z",
     "start_time": "2019-01-31T20:42:26.939373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_path = [str(x) for x in (list(data.train_ds.items)+list(data.valid_ds.items))]\n",
    "feature_dict = dict(zip(img_path,sf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:42:34.709806Z",
     "start_time": "2019-01-31T20:42:34.457238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Exporting as pickle\n",
    "pickle.dump(feature_dict, open(path+\"feature_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using Locality Sensitive hashing to find near similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:42:36.300586Z",
     "start_time": "2019-01-31T20:42:36.113779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Loading Feature dictionary\n",
    "feature_dict = pickle.load(open(path+'feature_dict.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T20:42:50.110319Z",
     "start_time": "2019-01-31T20:42:48.496286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Locality Sensitive Hashing\n",
    "# params\n",
    "k = 10 # hash size\n",
    "L = 5  # number of tables\n",
    "d = 512 # Dimension of Feature vector\n",
    "lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n",
    "\n",
    "# LSH on all the images\n",
    "for img_path, vec in notebook.tqdm(feature_dict.items()):\n",
    "    lsh.index(vec.flatten(), extra_data=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:09:26.945663Z",
     "start_time": "2019-01-31T21:09:26.719402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Exporting as pickle\n",
    "pickle.dump(lsh, open(path+'lsh.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:49.951305Z",
     "start_time": "2019-01-31T21:12:49.350659Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Feature dictionary\n",
    "feature_dict = pickle.load(open(path+'feature_dict.p','rb'))\n",
    "lsh = pickle.load(open(path+'lsh.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lsh.query(feature_dict[list(feature_dict.keys())[0]].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_item(idx, feature_dict, lsh_variable, n_items=5):\n",
    "    response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), \n",
    "                     num_results=n_items+1, distance_func='hamming')\n",
    "    \n",
    "    columns = 3\n",
    "    rows = int(np.ceil(n_items+1/columns))\n",
    "    fig=plt.figure(figsize=(2*rows, 3*rows))\n",
    "    for i in range(1, columns*rows +1):\n",
    "        if i<n_items+2:\n",
    "            img = Image.open(response[i-1][0][1])\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(img)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:51.836645Z",
     "start_time": "2019-01-31T21:12:50.928916Z"
    }
   },
   "outputs": [],
   "source": [
    "get_similar_item(0, feature_dict, lsh,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:54.383642Z",
     "start_time": "2019-01-31T21:12:53.621125Z"
    }
   },
   "outputs": [],
   "source": [
    "get_similar_item(50, feature_dict, lsh,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:55.074925Z",
     "start_time": "2019-01-31T21:12:54.386344Z"
    }
   },
   "outputs": [],
   "source": [
    "get_similar_item(20, feature_dict, lsh, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T21:12:57.393212Z",
     "start_time": "2019-01-31T21:12:55.749581Z"
    }
   },
   "outputs": [],
   "source": [
    "get_similar_item(30, feature_dict, lsh,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_item(100, feature_dict, lsh,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_item(90, feature_dict, lsh,11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
